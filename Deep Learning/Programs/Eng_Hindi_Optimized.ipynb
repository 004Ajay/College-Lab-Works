{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5pSpRekvPQf",
        "outputId": "7b7ce168-04b3-41e0-bcf5-74c4666a1486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, LSTM, Embedding, Dense\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/English_Hindi_Clean_New.csv\", encoding='utf-8')"
      ],
      "metadata": {
        "id": "7rdzn8uxvRTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get English and Hindi Vocabulary\n",
        "all_eng_words = set()\n",
        "for eng in data['English']:\n",
        "    for word in eng.split():\n",
        "        if word not in all_eng_words:\n",
        "            all_eng_words.add(word)\n",
        "\n",
        "all_hin_words = set()\n",
        "for hin in data['Hindi']:\n",
        "    for word in hin.split():\n",
        "        if word not in all_hin_words:\n",
        "            all_hin_words.add(word)"
      ],
      "metadata": {
        "id": "auzD2irhvhYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['len_eng_sen'] = data['English'].apply(lambda x: len(x.split(\" \")))\n",
        "data['len_hin_sen'] = data['Hindi'].apply(lambda x: len(x.split(\" \")))\n",
        "\n",
        "data = data[data['len_eng_sen'] <= 20]\n",
        "data = data[data['len_hin_sen'] <= 20]\n",
        "\n",
        "max_len_src = max(data['len_hin_sen'])\n",
        "max_len_tar = max(data['len_eng_sen'])\n",
        "\n",
        "inp_words = sorted(list(all_eng_words))\n",
        "tar_words = sorted(list(all_hin_words))\n",
        "num_enc_toks = len(all_eng_words)\n",
        "num_dec_toks = len(all_hin_words) + 1  # for zero padding\n",
        "\n",
        "inp_tok_idx = dict((word, i + 1) for i, word in enumerate(inp_words))\n",
        "tar_tok_idx = dict((word, i + 1) for i, word in enumerate(tar_words))\n",
        "rev_inp_char_idx = dict((i, word) for word, i in inp_tok_idx.items())\n",
        "rev_tar_char_idx = dict((i, word) for word, i in tar_tok_idx.items())"
      ],
      "metadata": {
        "id": "Wz_MgHBBvhWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test\n",
        "X, y = data['English'], data['Hindi']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Increase batch size\n",
        "batch_size = 256\n",
        "\n",
        "# Generate batch data\n",
        "def generate_batch(X=X_train, y=y_train, batch_size=batch_size):\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            enc_inp_data = np.zeros((batch_size, max_len_src), dtype='float32')\n",
        "            dec_inp_data = np.zeros((batch_size, max_len_tar), dtype='float32')\n",
        "            dec_tar_data = np.zeros((batch_size, max_len_tar, num_dec_toks), dtype='float32')\n",
        "            for i, (inp_text, tar_text) in enumerate(zip(X[j:j + batch_size], y[j:j + batch_size])):\n",
        "                for t, word in enumerate(inp_text.split()):\n",
        "                    enc_inp_data[i, t] = inp_tok_idx[word]\n",
        "                for t, word in enumerate(tar_text.split()):\n",
        "                    if t < len(tar_text.split()) - 1:\n",
        "                        dec_inp_data[i, t] = tar_tok_idx[word]\n",
        "                    if t > 0:\n",
        "                        dec_tar_data[i, t - 1, tar_tok_idx[word]] = 1.0\n",
        "            yield [enc_inp_data, dec_inp_data], dec_tar_data"
      ],
      "metadata": {
        "id": "MFKnG6eUvhT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder-Decoder Architecture\n",
        "latent_dim = 250\n",
        "\n",
        "# Encoder\n",
        "enc_inps = Input(shape=(None,))\n",
        "enc_emb = Embedding(num_enc_toks, latent_dim, mask_zero=True)(enc_inps)\n",
        "enc_lstm = LSTM(latent_dim, return_state=True)\n",
        "enc_outputs, st_h, st_c = enc_lstm(enc_emb)\n",
        "enc_states = [st_h, st_c]\n",
        "\n",
        "# Set up the decoder\n",
        "dec_inps = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_dec_toks, latent_dim, mask_zero=True)\n",
        "dec_emb = dec_emb_layer(dec_inps)\n",
        "dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "dec_outputs, _, _ = dec_lstm(dec_emb, initial_state=enc_states)\n",
        "dec_dense = Dense(num_dec_toks, activation='softmax')\n",
        "dec_outputs = dec_dense(dec_outputs)"
      ],
      "metadata": {
        "id": "96xtjos8vhPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Model([enc_inps, dec_inps], dec_outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')  # Use Adam optimizer for faster convergence"
      ],
      "metadata": {
        "id": "CKhGVmKEvhM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples = len(X_train)\n",
        "val_samples = len(X_test)\n",
        "\n",
        "# Train the model with a larger batch size\n",
        "model.fit(x=generate_batch(X_train, y_train, batch_size=batch_size),\n",
        "          steps_per_epoch=train_samples // batch_size,\n",
        "          epochs=50,\n",
        "          validation_data=generate_batch(X_test, y_test, batch_size=batch_size),\n",
        "          validation_steps=val_samples // batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP0dtlghvhKb",
        "outputId": "e3d094db-cd1d-4f8d-be2e-ea3b53d93909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "48/48 [==============================] - 51s 783ms/step - loss: 7.5451 - val_loss: 6.5352\n",
            "Epoch 2/50\n",
            "48/48 [==============================] - 22s 462ms/step - loss: 6.3473 - val_loss: 6.4173\n",
            "Epoch 3/50\n",
            "48/48 [==============================] - 23s 481ms/step - loss: 6.1831 - val_loss: 6.2896\n",
            "Epoch 4/50\n",
            "48/48 [==============================] - 28s 582ms/step - loss: 6.0273 - val_loss: 6.2288\n",
            "Epoch 5/50\n",
            "48/48 [==============================] - 22s 465ms/step - loss: 5.9056 - val_loss: 6.1855\n",
            "Epoch 6/50\n",
            "48/48 [==============================] - 26s 545ms/step - loss: 5.7906 - val_loss: 6.1191\n",
            "Epoch 7/50\n",
            "48/48 [==============================] - 25s 524ms/step - loss: 5.6637 - val_loss: 6.0332\n",
            "Epoch 8/50\n",
            "48/48 [==============================] - 21s 443ms/step - loss: 5.5399 - val_loss: 5.9620\n",
            "Epoch 9/50\n",
            "48/48 [==============================] - 25s 534ms/step - loss: 5.4213 - val_loss: 5.9064\n",
            "Epoch 10/50\n",
            "48/48 [==============================] - 24s 505ms/step - loss: 5.3143 - val_loss: 5.8633\n",
            "Epoch 11/50\n",
            "48/48 [==============================] - 25s 527ms/step - loss: 5.2139 - val_loss: 5.8283\n",
            "Epoch 12/50\n",
            "48/48 [==============================] - 26s 535ms/step - loss: 5.1189 - val_loss: 5.8034\n",
            "Epoch 13/50\n",
            "48/48 [==============================] - 21s 441ms/step - loss: 5.0321 - val_loss: 5.7862\n",
            "Epoch 14/50\n",
            "48/48 [==============================] - 25s 534ms/step - loss: 4.9511 - val_loss: 5.7785\n",
            "Epoch 15/50\n",
            "48/48 [==============================] - 26s 541ms/step - loss: 4.8778 - val_loss: 5.7839\n",
            "Epoch 16/50\n",
            "48/48 [==============================] - 22s 456ms/step - loss: 4.8124 - val_loss: 5.7871\n",
            "Epoch 17/50\n",
            "48/48 [==============================] - 21s 442ms/step - loss: 4.7426 - val_loss: 5.7861\n",
            "Epoch 18/50\n",
            "48/48 [==============================] - 24s 507ms/step - loss: 4.6713 - val_loss: 5.7769\n",
            "Epoch 19/50\n",
            "48/48 [==============================] - 22s 463ms/step - loss: 4.5944 - val_loss: 5.7889\n",
            "Epoch 20/50\n",
            "48/48 [==============================] - 21s 436ms/step - loss: 4.5201 - val_loss: 5.7861\n",
            "Epoch 21/50\n",
            "48/48 [==============================] - 25s 517ms/step - loss: 4.4463 - val_loss: 5.8041\n",
            "Epoch 22/50\n",
            "48/48 [==============================] - 25s 528ms/step - loss: 4.3754 - val_loss: 5.7915\n",
            "Epoch 23/50\n",
            "48/48 [==============================] - 25s 529ms/step - loss: 4.3051 - val_loss: 5.7878\n",
            "Epoch 24/50\n",
            "48/48 [==============================] - 24s 508ms/step - loss: 4.2389 - val_loss: 5.8110\n",
            "Epoch 25/50\n",
            "48/48 [==============================] - 26s 536ms/step - loss: 4.1640 - val_loss: 5.7997\n",
            "Epoch 26/50\n",
            "48/48 [==============================] - 25s 514ms/step - loss: 4.0849 - val_loss: 5.8001\n",
            "Epoch 27/50\n",
            "48/48 [==============================] - 21s 434ms/step - loss: 4.0092 - val_loss: 5.8060\n",
            "Epoch 28/50\n",
            "48/48 [==============================] - 21s 444ms/step - loss: 3.9322 - val_loss: 5.8143\n",
            "Epoch 29/50\n",
            "48/48 [==============================] - 24s 508ms/step - loss: 3.8587 - val_loss: 5.8389\n",
            "Epoch 30/50\n",
            "48/48 [==============================] - 21s 430ms/step - loss: 3.7950 - val_loss: 5.8481\n",
            "Epoch 31/50\n",
            "48/48 [==============================] - 22s 454ms/step - loss: 3.7296 - val_loss: 5.8450\n",
            "Epoch 32/50\n",
            "48/48 [==============================] - 21s 443ms/step - loss: 3.6546 - val_loss: 5.8638\n",
            "Epoch 33/50\n",
            "48/48 [==============================] - 25s 530ms/step - loss: 3.5757 - val_loss: 5.8893\n",
            "Epoch 34/50\n",
            "48/48 [==============================] - 21s 446ms/step - loss: 3.4986 - val_loss: 5.8865\n",
            "Epoch 35/50\n",
            "48/48 [==============================] - 25s 532ms/step - loss: 3.4274 - val_loss: 5.8968\n",
            "Epoch 36/50\n",
            "48/48 [==============================] - 26s 551ms/step - loss: 3.3493 - val_loss: 5.9381\n",
            "Epoch 37/50\n",
            "48/48 [==============================] - 21s 437ms/step - loss: 3.2765 - val_loss: 5.9645\n",
            "Epoch 38/50\n",
            "48/48 [==============================] - 22s 455ms/step - loss: 3.2058 - val_loss: 5.9726\n",
            "Epoch 39/50\n",
            "48/48 [==============================] - 22s 460ms/step - loss: 3.1447 - val_loss: 5.9890\n",
            "Epoch 40/50\n",
            "48/48 [==============================] - 25s 532ms/step - loss: 3.0753 - val_loss: 6.0238\n",
            "Epoch 41/50\n",
            "48/48 [==============================] - 25s 530ms/step - loss: 3.0102 - val_loss: 6.0369\n",
            "Epoch 42/50\n",
            "48/48 [==============================] - 21s 445ms/step - loss: 2.9462 - val_loss: 6.0570\n",
            "Epoch 43/50\n",
            "48/48 [==============================] - 25s 519ms/step - loss: 2.8728 - val_loss: 6.0498\n",
            "Epoch 44/50\n",
            "48/48 [==============================] - 21s 437ms/step - loss: 2.8025 - val_loss: 6.0810\n",
            "Epoch 45/50\n",
            "48/48 [==============================] - 25s 538ms/step - loss: 2.7322 - val_loss: 6.0960\n",
            "Epoch 46/50\n",
            "48/48 [==============================] - 22s 454ms/step - loss: 2.6622 - val_loss: 6.1224\n",
            "Epoch 47/50\n",
            "48/48 [==============================] - 21s 443ms/step - loss: 2.5919 - val_loss: 6.1690\n",
            "Epoch 48/50\n",
            "48/48 [==============================] - 26s 542ms/step - loss: 2.5253 - val_loss: 6.1852\n",
            "Epoch 49/50\n",
            "48/48 [==============================] - 25s 528ms/step - loss: 2.4598 - val_loss: 6.1972\n",
            "Epoch 50/50\n",
            "48/48 [==============================] - 21s 444ms/step - loss: 2.4195 - val_loss: 6.2292\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7893f4a13b20>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the input sequence to get the \"thought vectors\"\n",
        "enc_model = Model(enc_inps, enc_states)\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "dec_st_inp_h = Input(shape=(latent_dim,))\n",
        "dec_st_inp_c = Input(shape=(latent_dim,))\n",
        "dec_states_inps = [dec_st_inp_h, dec_st_inp_c]\n",
        "\n",
        "dec_emb2= dec_emb_layer(dec_inps) # Get the embeddings of the decoder sequence\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "dec_outputs2, st_h2, st_c2 = dec_lstm(dec_emb2, initial_state=dec_states_inps)\n",
        "dec_states2 = [st_h2, st_c2]\n",
        "dec_outputs2 = dec_dense(dec_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "\n",
        "# Final decoder model\n",
        "dec_model = Model(\n",
        "    [dec_inps] + dec_states_inps,\n",
        "    [dec_outputs2] + dec_states2)"
      ],
      "metadata": {
        "id": "OpBoxA2XvhH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(inp_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = enc_model.predict(inp_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    tar_seq = np.zeros((1,1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    tar_seq[0, 0] = tar_tok_idx['START_']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_cond = False\n",
        "    dec_sen = ''\n",
        "    while not stop_cond:\n",
        "        output_toks, h, c = dec_model.predict([tar_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_tok_idx = np.argmax(output_toks[0, -1, :])\n",
        "        sampled_char = rev_tar_char_idx[sampled_tok_idx]\n",
        "        dec_sen += ' '+sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '_END' or\n",
        "           len(dec_sen) > 50):\n",
        "            stop_cond = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        tar_seq = np.zeros((1,1))\n",
        "        tar_seq[0, 0] = sampled_tok_idx\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return dec_sen\n",
        "\n",
        "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
        "k=0\n",
        "(inp_seq, actual_output), _ = next(train_gen)\n",
        "hin_sen = translate(inp_seq)\n",
        "print(f'''Input English sentence: {X_train[k:k+1].values[0]}\\n\n",
        "          Predicted Hindi Translation: {hin_sen[:-4]}\\n\n",
        "          Actual Hindi Translation: {y_train[k:k+1].values[0][6:-4]}''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWKEeWXYvRQ1",
        "outputId": "741819b8-dcd9-47e0-ca6e-6051cd95ffa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Input English sentence: which is a pity but in india every other sport\n",
            "\n",
            "Actual Hindi Translation:  जिसपे हमें तरस आती है लेकिन भारत में हर खेल \n",
            "\n",
            "Predicted Hindi Translation:  जिसपे हमें तरस आती है कि भारत में एक व्यक्ति \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mWFPIUvmvROQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LFVg3CmrvRLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PTrZfRZVvRJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nnPW81AhvRG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-uCNhJUwvRDb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}